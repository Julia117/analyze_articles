{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_meduza = \"https://meduza.io/rss/all\"\n",
    "rss_vedomosti = \"https://www.vedomosti.ru/rss/news\"\n",
    "rss_lenta = \"https://lenta.ru/rss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_meduza = feedparser.parse(rss_meduza)\n",
    "feed_vedomosti = feedparser.parse(rss_vedomosti)\n",
    "feed_lenta = feedparser.parse(rss_lenta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "meduza_articles = []\n",
    "vedomosti_articles = []\n",
    "lenta_articles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in feed_meduza['items']:\n",
    "    if is_news(item):\n",
    "        meduza_articles.append(get_text(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in feed_vedomosti['items']:\n",
    "    vedomosti_articles.append(get_text(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in feed_lenta['items']:\n",
    "    lenta_articles.append(get_text(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_news(item):\n",
    "    if \"news\" in item['link']:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "def get_text(item):\n",
    "    url = item['link']\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(meduza_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yulialysenko/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Installing mystem to /Users/yulialysenko/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-macosx.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "#--------#\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n",
    "#Create lemmatizer and stopwords list\n",
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "#Preprocess function\n",
    "def preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "    \n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'асфальт мимо цемент избегать зевака аплодисменты обитатель спальный аррондисман'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(\"Ну что сказать, я вижу кто-то наступил на грабли, Ты разочаровал меня, ты был натравлен.\")\n",
    "#> 'сказать видеть кто-то наступать грабли разочаровывать натравлять'\n",
    "\n",
    "preprocess_text(\"По асфальту мимо цемента, Избегая зевак под аплодисменты. Обитатели спальных аррондисманов\")\n",
    "#> 'асфальт мимо цемент избегать зевака аплодисменты обитатель спальный аррондисман'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "##data = \"Mars is a cold desert world. It is half the size of Earth. \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mars is a cold desert world.', 'It is half the size of Earth.']\n",
      "['Mars', 'is', 'a', 'cold', 'desert', 'world', '.', 'It', 'is', 'half', 'the', 'size', 'of', 'Earth', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(data))\n",
    "print(word_tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 12\n"
     ]
    }
   ],
   "source": [
    "file_docs = []\n",
    "\n",
    "tokens = sent_tokenize(meduza_articles[0])\n",
    "for line in tokens:\n",
    "    file_docs.append(line)\n",
    "\n",
    "print(\"Number of documents:\",len(file_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in file_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Британские организации, работающие над созданием вакцины от COVID-19, подвергаются хакерским атакам со стороны других государств, в частности, Ирана, России и Китая. Об этом пишет The Guardian со ссылкой на данные Национального центра кибербезопасности.\n",
      "\n",
      "Как отмечает издание, ни одна из атак пока не была успешной.\n",
      "\n",
      "Во всем мире сейчас разрабатывается несколько десятков различных видов вакцин от COVID-19.\n",
      "\n",
      "В Великобритании дальше всех в разработке вакцины продвинулись ученые из Института Дженнера в Оксфорде. Разрабатываемый там препарат уже запустили в производство параллельно с началом испытаний на людях — в случае, если они окажутся успешными, уже к сентябрю ученые рассчитывают получить до миллиона доз.\n",
      "Британские университеты и научные организации, работающие над вакциной от коронавирусной инфекции, стали жертвами хакерских атак, пишет The Guardian со ссылкой на экспертов по кибербезопасности. «Считается, что за хакерскими атаками стоят государства, включая Иран и Россию, эксперты также называют Китай в качестве вероятного виновного», – пишет британская газета. О том, что кибератаки на британские учреждения, занимающиеся исследованиями нового коронавируса, можно отследить к России и Ирану, также сообщает таблоид The Mail on Sunday.\n"
     ]
    }
   ],
   "source": [
    "print(meduza_articles[18])\n",
    "print(vedomosti_articles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer(min_df=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = vect.fit_transform(file_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.05879216, 0.        , 0.        , 0.05047745,\n",
       "        0.        , 0.08639927, 0.0562797 , 0.        , 0.06356501,\n",
       "        0.1270475 , 0.        ],\n",
       "       [0.05879216, 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.06607413, 0.08264944, 0.        , 0.04307758, 0.03413778,\n",
       "        0.06514273, 0.        ],\n",
       "       [0.        , 0.        , 1.        , 0.        , 0.02974314,\n",
       "        0.        , 0.        , 0.        , 0.07762103, 0.02399968,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 1.        , 0.04096692,\n",
       "        0.06614297, 0.        , 0.        , 0.        , 0.04236234,\n",
       "        0.        , 0.        ],\n",
       "       [0.05047745, 0.        , 0.02974314, 0.04096692, 1.        ,\n",
       "        0.07209613, 0.        , 0.        , 0.        , 0.03603132,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.06607413, 0.        , 0.06614297, 0.07209613,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.08639927, 0.08264944, 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.09932989, 0.09182167, 0.07276611,\n",
       "        0.12819192, 0.        ],\n",
       "       [0.0562797 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.09932989, 1.        , 0.        , 0.05148283,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.04307758, 0.07762103, 0.        , 0.        ,\n",
       "        0.        , 0.09182167, 0.        , 1.        , 0.02430236,\n",
       "        0.07237211, 0.        ],\n",
       "       [0.06356501, 0.03413778, 0.02399968, 0.04236234, 0.03603132,\n",
       "        0.        , 0.07276611, 0.05148283, 0.02430236, 1.        ,\n",
       "        0.24391438, 0.        ],\n",
       "       [0.1270475 , 0.06514273, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.12819192, 0.        , 0.07237211, 0.24391438,\n",
       "        1.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_similarity = tfidf * tfidf.T \n",
    "pairwise_similarity.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000004\n",
      "0.21626697905977543\n"
     ]
    }
   ],
   "source": [
    "import nltk, string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "##nltk.download('punkt') # if necessary...\n",
    "\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "def normalize(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=normalize)\n",
    "\n",
    "def cosine_sim(text1, text2):\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]\n",
    "\n",
    "\n",
    "print (cosine_sim(preprocess_text(meduza_articles[0]), preprocess_text(meduza_articles[0])))\n",
    "print (cosine_sim(preprocess_text(meduza_articles[18]), preprocess_text(vedomosti_articles[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'британский организация работать создание вакцина covid 19 подвергаться хакерский атака сторона государство частность иран россия китай это писать the guardian ссылка данные национальный центр кибербезопасность отмечать издание атака пока успешный мир разрабатываться несколько десяток различный вид вакцина covid 19 великобритания далеко разработка вакцина продвигаться ученый институт дженнер оксфорд разрабатывать препарат запускать производство параллельно начало испытание человек  —  случай оказываться успешный сентябрь ученый рассчитывать получать миллион доза'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(meduza_articles[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'британский университет научный организация работать вакцина коронавирусный инфекция становиться жертва хакерский атака писать the guardian ссылка эксперт кибербезопасность  « считаться хакерский атака стоять государство включая иран россия эксперт также называть китай качество вероятный виновный », –  писать британский газета кибератака британский учреждение заниматься исследование новый коронавирус отслеживать россия иран также сообщать таблоид the mail on sunday'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(vedomosti_articles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'коронавирус': 1, 'михаила': 2, 'мишустина': 3, 'нашли': 4, 'премьер-министра': 5, 'россии': 6, 'у': 7, ',': 8, '«': 9, '»': 10, 'владимиру': 11, 'доложил': 12, 'интерфакс': 13, 'мишустин': 14, 'об': 15, 'путину': 16, 'сам': 17, 'сообщает': 18, 'этом': 19, 'заявил': 20, 'на': 21, 'самоизоляцию': 22, 'уходит': 23, 'что': 24, 'андрея': 25, 'белоусова': 26, 'исполняющим': 27, 'назначить': 28, 'обязанности': 29, 'он': 30, 'предложил': 31, 'премьера': 32, ';': 33, 'был': 34, 'кандидатуру': 35, 'минут': 36, 'несколько': 37, 'поддержал': 38, 'подписан': 39, 'путин': 40, 'соответствующий': 41, 'уже': 42, 'указ': 43, 'через': 44, 'эту': 45}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-9f68dde2ecab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-121-9f68dde2ecab>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "for doc in tf_idf[corpus]:\n",
    "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
